{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE-361M Introduction to Data Mining\n",
    "## Assignment #2\n",
    "## Due: Thursday, Feb 18, 2016 by 2pm; Total points: 50\n",
    "## Group 3\n",
    "## Members: Wenyang Fu, Rohan Nagar, Zuhair Parvez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 1: Sampling\n",
    "### 10 points\n",
    "\n",
    "1. CBS has come up with an extreme TV show, and each of its viewers either likes or hates it. (no middle ground here; we are in a 'black and white age'). CBS wants to estimate what fraction $p$ of its audience like the show by 'randomly' calling $n$ viewers and tallying their responses so as to estimate the true value of $p$ to a fractional  accuracy of within $\\pm \\epsilon$%, with a confidence of $(1-\\alpha) \\times 100$%. For $\\alpha =  0.1$, $\\epsilon = 0.02$ (i.e. your answer will be $\\hat{p} \\pm 0.02$), what is the minimum value of $n$ needed if (i) true value $p = 0.5$ and (ii) $p = 0.95$? \n",
    "%(First try to do this yourself knowing that you have a binomial distribution, which can be approximated by a normal distribution. If you cannot, consult an undergrad stats book.)\n",
    "2. Suppose for a certain value  of $p$ and choice of $\\epsilon$, you calculate that you will need (at least) 1000 samples for $\\alpha = 0.1$. You now decide to obtain  a more accurate answer by either (i) reducing $\\alpha$ to 0.05, keeping the same $\\epsilon$ or by (ii) reducing $\\epsilon$ by a factor of 2 from the original value, but maintaining  $\\alpha = 0.1$.  In each case how many samples would you need now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1.1\n",
    "By the central limit theorem, $X\\sim B(n, p=0.5)$ can only be approximated by a Gaussian when $n\\gt30$ and $np$, $n(1-p)\\gt5$. The smallest value of $n$ that will fulfill these conditions would be $31$. However, we also have to take into account the minimum number of samples we need to estimate the true proportion, denoted by the formula:\n",
    "\n",
    "$$n\\ge p(1-p) * \\bigg(\\frac{z_{\\alpha/2}}{\\epsilon}\\bigg)^2$$\n",
    "\n",
    "In this particular case, $z_{\\alpha/2} \\sim \\pm1.65, \\epsilon = .02,$ and $p=0.5$. Plugging these numbers in, we get:\n",
    "\n",
    "$$n\\ge .25 * \\bigg(\\frac{1.65}{.02}\\bigg)^2$$\n",
    "$$n\\ge 1701.5625$$\n",
    "\n",
    "Thus, the minimum number of samples we need to fulfill all conditions would be $n\\ge1702$.\n",
    "\n",
    "\n",
    "### Question 1.1.2\n",
    "By the CLT, $X\\sim B(n, p=0.95)$ can only be approximated by a Gaussian when $n\\gt30$ and $np$, $n(1-p)\\gt5$. The smallest value of $n$ that will fulfill these conditions would be $101$, calculated by computing $n(1-.95)\\gt5$. However, we also have to take into account the minimum number of samples we need to estimate the true proportion.\n",
    "In this particular case, $z_{\\alpha/2} \\sim \\pm1.65, \\epsilon = .02,$ and $p=0.95$. Then:\n",
    "\n",
    "$$n\\ge .95(1-.95) * \\bigg(\\frac{1.65}{.02}\\bigg)^2$$\n",
    "$$n\\ge 323.296875$$\n",
    "\n",
    "Thus, the minimum number of samples we need to fulfill all conditions would be $n\\ge324$.\n",
    "\n",
    "### Question 1.2\n",
    "We are given $n=1000$, $\\alpha = .1$, $z_{\\alpha/2}\\sim1.65$\n",
    "\n",
    "### Question 1.2.1\n",
    "\n",
    "Since $\\alpha$ changed to .05, $z_{\\alpha/2}\\sim1.96$. We must multiply both sides of our inequality by $(\\frac{1.96}{1.65})^2$, which means we now need $n=1412$ samples to estimate the true proportion.\n",
    "\n",
    "### Question 1.2.2\n",
    "\n",
    "Since we halved our $\\epsilon$, we will need to multiply both sides of our inequality by $(\\frac{1}{2})^2$, which means that we now need $n=4000$ samples to estimate the true proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Republican Presidental Debate\n",
    "### 10 points\n",
    "\n",
    "In this question we will be analyzing text data from one of the recent presidental debates. I have included code below to grab the data for you from the New York Times.\n",
    "\n",
    "1. Create a set of the frequency of utterance of  all the distinct words spoken by candidates, and then use it to create a histogram (with 30 bins) of word counts. Thus a bin is a range of count values and the corresponding \"y\" value is the number of words whose count falls in this range. What is interesting about this distribution? What are the 10 most common words?\n",
    "2. Remove the 100 most common words from vocabulary. Meaning that if you ever see this word, get rid of it. Now create a new python dictionary for each candidate that is a single list of all the words spoken by this candidate (ignoring these most common words). What are the 10 most common words for Trump, Rubio, and Cruz? How do their words differ?\n",
    "3. Using our dictionary from number 2, how many words did each speaker speak? Who spoke the most? Who is the outlier?\n",
    "4. Count the percentage of time each person uses the words (I, I'm, me, mine). When doing this convert all words to lower case. Create a bar plot of this percentage for each candidate with bars from largest to smallest. Use dictionary that has all words (doesn't exclude most common). What does the plot show?\n",
    "\n",
    "Hints:\n",
    "1. Look at python Counter.\n",
    "2. Just split text on a space. This isn't perfect, but will be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "url = 'http://www.nytimes.com/2015/11/11/us/politics/transcript-republican-presidential-debate.html'\n",
    "# requests gets the source code from the url and extracts it as text\n",
    "html = requests.get(url).text\n",
    "# beautifulsoup is a library that takes in text source code and returns a structured format of that\n",
    "# source code that you can more easily search and parse.\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "# get all the 'p' tags from the source with class = 'story-body-text'\n",
    "# this was determined by looking at the source code\n",
    "# the first and last paragraphs are intro and ending\n",
    "paragraphs = soup('p', {'class': 'story-body-text'})[1:-1]\n",
    "candidates = ['BUSH', 'TRUMP', 'RUBIO', \n",
    "              'CARSON', 'FIORINA', 'KASICH', 'CRUZ', 'PAUL']\n",
    "\n",
    "def text_to_dict(paragraph_array):\n",
    "    '''takes an array of text paragraphs from debate and returns dict \n",
    "    where key is person and value is list of text spoken by that candidate'''\n",
    "    # dict is like a hash map. defaultdict lets you specify what types of values will be in your hash map\n",
    "    d = defaultdict(list)\n",
    "    # just a default speaker that won't end up in our returned data\n",
    "    # will get replaced when an actual speaker is found\n",
    "    speaker = \"<START>\"\n",
    "    for paragraph in paragraph_array:\n",
    "        words = paragraph.text.split(' ')\n",
    "        first_word = words[0]\n",
    "        # only new speaker when have SPEAKER: format\n",
    "        if first_word[-1] == \":\":\n",
    "            speaker = first_word[:-1]\n",
    "        # only keep candidates text\n",
    "        if speaker in candidates:\n",
    "            d[speaker].append(words[1:])\n",
    "    return d\n",
    "\n",
    "speaker_dict = text_to_dict(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3310   47   15    7    4    2    0    0    2    2    2    0    0    2    1\n",
      "    0    0    0    0    1    0    0    0    0    0    0    0    0    0    1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAECCAYAAAAciLtvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x231a4d2e0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common words: [('the', 775), ('to', 508), ('a', 365), ('of', 360), ('and', 352), ('we', 267), ('that', 261), ('is', 258), ('in', 254), ('I', 232)]\n"
     ]
    }
   ],
   "source": [
    "# Question 2.1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def count_words(speaker_dict):\n",
    "    word_counts = Counter()\n",
    "    for k,v in speaker_dict.items(): \n",
    "        # every value in speaker_dict is a nested list of lists.\n",
    "        # use nested list comprehension to flatten the nested list\n",
    "        words = [word for paragraph in v for word in paragraph]\n",
    "        for word in words:\n",
    "            word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "word_counts = count_words(speaker_dict)\n",
    "# Unsorted word counts, now without keys\n",
    "counts = [v for k,v in word_counts.items()]\n",
    "hist, bin_edges = np.histogram(counts, bins=30)\n",
    "print(hist)\n",
    "plt.hist(counts, bins=30)\n",
    "plt.show()\n",
    "print(\"10 most common words: {}\".format(word_counts.most_common(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "The distribution is interesting because it is extremely long-tailed. Most words are spoken with low frequency, but a few words (mainly particles and pronouns) are spoken with extremely high frequency. The ten most common words were (word,count):\n",
    "\n",
    "[('the', 775), ('to', 508), ('a', 365), ('of', 360), ('and', 352), ('we', 267), ('that', 261), ('is', 258), ('in', 254), ('I', 232)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate TRUMP's ten most common words are:[('country', 10), ('come', 9), ('country.', 8), ('ever', 8), ('right', 6), ('nobody', 6), ('tell', 6), ('jobs', 5), ('United', 5), ('And,', 5)]\n",
      "Candidate RUBIO's ten most common words are:[('important', 9), ('job', 8), ('new', 8), ('ever', 8), ('21st', 7), ('most', 7), ('Americans', 6), ('nation', 6), ('better', 6), ('American', 6)]\n",
      "Candidate CRUZ's ten most common words are:[('which', 9), ('economic', 9), ('her', 7), ('Washington', 7), ('tax,', 6), ('pay', 6), ('youâ€™re', 6), ('that,', 6), ('And,', 6), ('put', 6)]\n"
     ]
    }
   ],
   "source": [
    "# Question 2.2\n",
    "\n",
    "# 100 most common words, which we will need to remove.\n",
    "top_words = [word for (word,count) in word_counts.most_common(100)]\n",
    "\n",
    "def candidate_words_filtered(speaker_dict):\n",
    "    words_by_candidate = defaultdict(list)\n",
    "    for k,v in speaker_dict.items(): \n",
    "        # every value in speaker_dict is a nested list of lists.\n",
    "        # use nested list comprehension to flatten the nested list\n",
    "        words = [word for paragraph in v for word in paragraph]\n",
    "        # Remove the 100 most common words from the list of all words\n",
    "        words_no_outliers = [word for word in words if not any(outlier == word for outlier in top_words)]\n",
    "#         print(len(words))\n",
    "#         print(len(words_no_outliers))\n",
    "        for word in words_no_outliers:\n",
    "            if k in words_by_candidate:\n",
    "                words_by_candidate[k].append(word)\n",
    "            else:\n",
    "                words_by_candidate[k] = [word]\n",
    "    return words_by_candidate\n",
    "\n",
    "words_by_candidates = candidate_words_filtered(speaker_dict)\n",
    "\n",
    "def top_ten_words(candidate_name):\n",
    "    word_counts = Counter(words_by_candidates[candidate_name])\n",
    "    top_ten = word_counts.most_common(10)\n",
    "    print(\"Candidate {}'s ten most common words are:{}\".format(candidate_name, top_ten))\n",
    "\n",
    "top_ten_words(\"TRUMP\")\n",
    "top_ten_words(\"RUBIO\")\n",
    "top_ten_words(\"CRUZ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate PAUL spoke 828 words\n",
      "Candidate BUSH spoke 972 words\n",
      "Candidate TRUMP spoke 984 words\n",
      "Candidate FIORINA spoke 1000 words\n",
      "Candidate CARSON spoke 679 words\n",
      "Candidate CRUZ spoke 1131 words\n",
      "Candidate KASICH spoke 1099 words\n",
      "Candidate RUBIO spoke 1124 words\n",
      "Candidate CRUZ spoke the most words with a count of 1131 words\n"
     ]
    }
   ],
   "source": [
    "# Question 2.3\n",
    "\n",
    "def candidate_word_count(candidate_name):\n",
    "    return Counter(words_by_candidates[candidate_name])\n",
    "\n",
    "candidate_counters = {name: candidate_word_count(name) for name in candidates}\n",
    "most_talkative = \"\"\n",
    "most_words = 0\n",
    "for name, counter in candidate_counters.items():\n",
    "    num_words_spoken = sum(counter.values())\n",
    "    if num_words_spoken > most_words:\n",
    "        most_words = num_words_spoken\n",
    "        most_talkative = name\n",
    "    print(\"Candidate {} spoke {} words\".format(name, num_words_spoken))\n",
    "print(\"Candidate {} spoke the most words with a count of {} words\".format(most_talkative, most_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "Candidate FIORINA spoke 1000 words.\n",
    "Candidate RUBIO spoke 1124 words.\n",
    "Candidate CRUZ spoke 1131 words.\n",
    "Candidate KASICH spoke 1099 words.\n",
    "Candidate PAUL spoke 828 words.\n",
    "Candidate BUSH spoke 972 words.\n",
    "Candidate TRUMP spoke 984 words.\n",
    "Candidate CARSON spoke 679 words.\n",
    "Candidate CRUZ spoke the most words with a count of 1131 words.\n",
    "It looks like Carson is the outlier, speaking the least with only 679 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('KASICH', 2.310654685494223), ('CARSON', 2.154046997389034), ('TRUMP', 2.1390374331550803), ('PAUL', 1.698886936145284), ('FIORINA', 1.600413009808983), ('RUBIO', 1.1970927746900384), ('CRUZ', 1.150483202945237), ('BUSH', 0.7272727272727273)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAECCAYAAAAB2kexAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngljqklSQ/wUWU51A4nQXHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x231a765ecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question 2.4\n",
    "from __future__ import division\n",
    "\n",
    "def candidate_words(speaker_dict):\n",
    "    words_by_candidate = defaultdict(list)\n",
    "    for k,v in speaker_dict.items(): \n",
    "        # every value in speaker_dict is a nested list of lists.\n",
    "        # use nested list comprehension to flatten the nested list\n",
    "        words = [word.lower() for paragraph in v for word in paragraph]\n",
    "        for word in words:\n",
    "            if k in words_by_candidate:\n",
    "                words_by_candidate[k].append(word)\n",
    "            else:\n",
    "                words_by_candidate[k] = [word]\n",
    "    return words_by_candidate\n",
    "\n",
    "words_by_candidates = candidate_words(speaker_dict)\n",
    "candidate_counters = {name: candidate_word_count(name) for name in candidates}\n",
    "\n",
    "def narcissist_frequency(candidate_name):\n",
    "    # Calculates the % of time each person uses words\n",
    "    counter = candidate_counters[candidate_name]\n",
    "    narcissist_words = ['i\\'m', 'i', 'me', 'mine']\n",
    "    narcissism_count = sum([counter[word] for word in narcissist_words])\n",
    "    total_count = sum(counter.values())\n",
    "    return 100* narcissism_count / total_count\n",
    "\n",
    "# Need to print frequencies in descending order\n",
    "narcissism_freq = Counter({name: narcissist_frequency(name) for name in candidates})\n",
    "y_vals = [freq[1] for freq in narcissism_freq.most_common()]\n",
    "print(narcissism_freq.most_common())\n",
    "plt.bar(range(len(y_vals)), y_vals)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4\n",
    "Kasich talks the most about himself, while Bush talks the least about himself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queston 3: Principal Component Analysis\n",
    "### 15 points\n",
    "\n",
    "In this question, you will explore an application of PCA.\n",
    "\n",
    "1. Convert your data from 3.2 to a vectorized format. This means you will have a row for each candidate and a column for each word in your data. A column for a candidate will contain the number of times that candidate used that word. Use [CountVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html) from sklearn with min_df = 1.\n",
    "2. Convert your data from a sparse matrix to a dense array using .toarray() and then scale it to have mean zero and standard deviation of 1. See [here](http://scikit-learn.org/stable/modules/preprocessing.html) for help.\n",
    "2. Plot the explained variance as a function of the number of PCA components (called a scree plot). Use sklearn's PCA functionality to do this.\n",
    "3. Now pick the top two principal components and project the data onto the respective dimensions. Visualize the data in a scatter plot and label each point with the candidate's name. Who are the outliers? Use sklearn and matplotlib for this. \n",
    "4.  In what sense is PCA an optimal feature extraction technique? Describe a situation where you would prefer feature selection to (linear) feature extraction, even though the former  is a special case of the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Robust Regression\n",
    "### 5 points\n",
    "\n",
    "In this question we will be exploring using a regression technique that is more robust to outliers. I provide some code below that injects outlier points into the original medv and lstat data from the housing dataset. This problem looks into how robust regression can help in the presence of outliers.\n",
    "\n",
    "1. Using the original data, plot lstat on the x-axis and log(medv) on the y-axis of a scatter plot with the line of best fit from a linear regression on the plot as well. Do the same, but with the data that includes the outlier values. What changes with the best fit line? Specifically, how does the slope change?\n",
    "2. Now run a linear regression with a Huber loss on the data including the outliers and create the same plot as above, but this time with the fit from the Huber loss regression (using all the data). What has changed (comment on the slope as well)? Note: Use SGDRegressor from sklearn with 500 iterations and no penalty.\n",
    "3. Explain why the huber loss is more robust to outliers.\n",
    "\n",
    "Note:  Use plot's with xlim = (-5, 40) and ylim = (1, 5). These set the range for the x any y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "housing_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n",
    "                   delim_whitespace=True, header=None,\n",
    "                   names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "                           'B', 'LSTAT', 'MEDV'])\n",
    "housing_data = housing_data.dropna()\n",
    "lstat = housing_data.LSTAT.values\n",
    "medv = housing_data.MEDV.values\n",
    "medv_std = np.std(medv)\n",
    "lstat_std = np.std(lstat)\n",
    "np.random.seed(42)\n",
    "medv_outliers = np.random.normal(1, 1, 5)\n",
    "lstat_outliers = np.random.normal(1, 1, 5)\n",
    "medv_with_outliers = np.append(medv, medv_outliers)\n",
    "lstat_with_outliers = np.append(lstat, lstat_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Visualization using Bokeh\n",
    "## 10 points\n",
    "\n",
    "In this problem, you'll build an interactive visualization. Bokeh is a Python interactive visualization library that targets modern web browsers for presentation. For more information on Bokeh, see http://bokeh.pydata.org/en/latest/. The problem statement is as follows:\n",
    "\n",
    "Using the [auto-mpg](http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original) data, your goal is to build a Bokeh visualization which allows the user explore how MPG varies with horsepower and weight. You will create a visualization that allows the user to toggle the Y axis of a scatter plot between horsepower and weight. With the x-axis always being MPG.\n",
    "\n",
    "Hints: \n",
    "1. You can make use of Select widgets.\n",
    "2. See: http://bokeh.pydata.org/en/latest/docs/user_guide/interaction.html#javascript-callbacks. Specifically look at the CustomJS for Widgets under Callbacks and the Select widget. \n",
    "3. See: http://bokeh.pydata.org/en/latest/docs/reference/plotting.html. Look for the scatter API.\n",
    "4. See: http://bokeh.pydata.org/en/0.10.0/docs/user_guide/styling.html#labels. For labeling axes.\n",
    "5. Use output_notebook() from Bokeh to output the plot to your notebook\n",
    "\n",
    "We have made available a sample screenshot of our Bokeh app that supports the above requirements. Your interface should look similar to the screenshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
