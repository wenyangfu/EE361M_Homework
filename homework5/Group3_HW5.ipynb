{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE361M Introduction to Data Mining - Assignment #5\n",
    "## Rohan Nagar, Wenyang Fu, Zuhair Parvez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (1+1+5+3+3+2=15pts) - Logistic Regression\n",
    "\n",
    "In this question we will be predicting whether someone will have an affair! Yes - there is data on this. See below on how to import the data.\n",
    "1. Convert naffairs to a binary variable hadAffair which is 1 if had an affair and zero otherwise\n",
    "2. Split the data into training and test. Use 42 as random seed and use 1/3rd of the data for testing. Our y variable is hadAffair and X matrix includes all the other variables except naffairs.\n",
    "3. Train a logistic regression with almost no regularization (pass l2 (ridge) to penalty and 1,000 to the C parameter which is the inverse of regularization strength lambda. This essentially does l2 regularization but applies very little weight to the penalty term) and report the confusion matrix on the test data. Also report the accuracy for the \"no affairs\" class, the affairs class, and the average per-class accuracy on the test data. Average per-class accuracy is described in this [post](http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2a-classification-metrics).\n",
    "4. Repeat step 3 except use l2 penalty with Cs of [.001, .01,0.1, 1]. You will want to use k-fold cross validation to select the best parameter. To evaluate which parameter is best, maximize the average per-class accuracy. To help with this task, check out [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) and how to make your own [custom scorer](http://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "5. Repeat question 4 except use l1 (i.e. Lasso) instead of l2 as the penalty type.\n",
    "6. Which model produces the best average per-class accuracy? Why do you think this is the case? How do the models handle the different classes, and why is this so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "df = data('affairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as get_confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def custom_score_function(ground_truth, predictions):\n",
    "    confusion_matrix = get_confusion_matrix(ground_truth, predictions)\n",
    "    no_affairs_acc = confusion_matrix[0,0]/(sum(confusion_matrix[0]))\n",
    "    affairs_acc = confusion_matrix[1,1]/(sum(confusion_matrix[1]))\n",
    "    return (no_affairs_acc + affairs_acc) / 2\n",
    "\n",
    "def print_metrics(ground_truth, predicted):\n",
    "    confusion_matrix = get_confusion_matrix(Y_test, Y_predict)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    no_affairs_acc = confusion_matrix[0,0]/(sum(confusion_matrix[0]))\n",
    "    print(\"No affairs accuracy: %0.02f\" % no_affairs_acc)\n",
    "\n",
    "    affairs_acc = confusion_matrix[1,1]/(sum(confusion_matrix[1]))\n",
    "    print(\"Affairs accuracy: %0.02f\" % affairs_acc)\n",
    "\n",
    "    average_acc = (no_affairs_acc + affairs_acc) / 2\n",
    "    print(\"Average accuracy: %0.02f\" % average_acc)\n",
    "\n",
    "scorer = make_scorer(custom_score_function, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1 and 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "df['hadAffair'] = df['naffairs'] > 0\n",
    "\n",
    "features = df.columns[1:-1] # naffairs is index 0 and hadAffair is -1\n",
    "X = df[features].values\n",
    "Y = df['hadAffair'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[136  10]\n",
      " [ 43  10]]\n",
      "No affairs accuracy: 0.93\n",
      "Affairs accuracy: 0.19\n",
      "Average accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "l2_classifier = LogisticRegression(C=1000, penalty='l2').fit(X_train, Y_train)\n",
    "Y_predict = l2_classifier.predict(X_test)\n",
    "\n",
    "print_metrics(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[137   9]\n",
      " [ 43  10]]\n",
      "No affairs accuracy: 0.94\n",
      "Affairs accuracy: 0.19\n",
      "Average accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "parameters = [{'C' : [.001, .01,0.1, 1], 'penalty':['l2']}]\n",
    "\n",
    "classifier = GridSearchCV(LogisticRegression(), parameters, scoring=scorer)\n",
    "classifier = classifier.fit(X_train, Y_train)\n",
    "\n",
    "Y_predict = classifier.predict(X_test)\n",
    "\n",
    "print_metrics(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[138   8]\n",
      " [ 43  10]]\n",
      "No affairs accuracy: 0.95\n",
      "Affairs accuracy: 0.19\n",
      "Average accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "parameters = [{'C' : [.001, .01,0.1, 1], 'penalty':['l1']}]\n",
    "\n",
    "classifier = GridSearchCV(LogisticRegression(), parameters, scoring=scorer)\n",
    "classifier = classifier.fit(X_train, Y_train)\n",
    "\n",
    "Y_predict = classifier.predict(X_test)\n",
    "\n",
    "print_metrics(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Answer this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (2+3+2+3=10pts) - Support Vector Classifier\n",
    "\n",
    "This question will continue to use the data from question 1 - including the training and test split data.\n",
    "1. Fit a support vector classifier using the standard options on [sklearn's SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). Report the confusion matrix on the test data. Also report the accuracy for the no affairs class, the affairs class, and the average per-class accuracy (same as question 1).\n",
    "2. Repeat question 1 except use grid search to select the best value of C within this set: [0.001, 0.01, 0.1, 1,5,10,100] and try both a radial and polynomial kernel (thus trying 14 combinations). Choose the combination that maximizes the average per-class accuracy. Use 5 folds. Report the best model, the confusion matrix, the accuracy for the no affairs class, the affairs class, and the average per-class accuracy.\n",
    "3. Briefly discuss the effect of different  C,  kernel combinations.\n",
    "4. Discuss your results from parts 1 and 2 and mention how they differ from Question 1's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[146   0]\n",
      " [ 53   0]]\n",
      "No affairs accuracy: 1.00\n",
      "Affairs accuracy: 0.00\n",
      "Average accuracy: 0.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC()\n",
    "Y_predict = classifier.fit(X_train, Y_train).predict(X_test)\n",
    "print_metrics(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[119  27]\n",
      " [ 37  16]]\n",
      "No affairs accuracy: 0.82\n",
      "Affairs accuracy: 0.30\n",
      "Average accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "search_params = [{'C':[0.001, 0.01, 0.1, 1, 5, 10, 100], 'kernel':['rbf','poly']}]\n",
    "\n",
    "classifier = GridSearchCV(SVC(), search_params, scoring=scorer, cv=5)\n",
    "classifier = classifier.fit(X_train, Y_train)\n",
    "\n",
    "Y_predict = classifier.predict(X_test)\n",
    "\n",
    "print_metrics(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "The C parameter (our slack penalty) in the SVC model reflects a tradeoff between reducing misclassification and model simplicity. With a lower C value, the decision surface is smooth, but there are potentially more misclassifications. With a high C value, the model will try to classify all training data correctly by giving the model more freedom to select more support vectors. This parameter also represents the bias-variance tradeoff.\n",
    "\n",
    "The kernel function allows us to map data into a high-dimensional space and compute the inner product - allowing us to find a hyperplane to seperate originally nonlinearally seperable data. Different kernels will try to map the data to the higher-dimensional space with different funtions. For example, we used RBF and Polynomial, and those functions can be viewed here:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Radial_basis_function_kernel\n",
    "\n",
    "https://en.wikipedia.org/wiki/Polynomial_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4\n",
    "\n",
    "In part 1, we can see that the SVM classified everything as 'no affairs'. This obviously makes the 'no affairs' accuracy very good, but the average per-class accuracy is bad (0.50).\n",
    "\n",
    "When we select the best C and kernel parameters, we get a much better SVM classifier. It looks similar to our answers in Question 1 in terms of the average per-class accuracy. One thing we can notice is that the 'affairs' accuracy actually performed a lot better than our Logistic Regression models in Q1, while our 'no affairs' accuracy did slightly worse. This is interesting to note, that our SVM was able to do a little better on classifing affairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 (2+1+3+1+3=10pts) - Regression Trees\n",
    "\n",
    "This question is very similar to homework 4 question 1. Except now we will be using regression trees and not classification trees, and you will be addressing a regression problem (i.e., the independent variable \"price\" will not be binarized).\n",
    "\n",
    "For this question, we will be using the housing dataset (see code below). \n",
    "\n",
    "1. Convert driveway, recroom, fullbase, gashw, airco, and prefarea to numeric dummy variables (1 for yes, zero for no)\n",
    "2. Split the data into training and testing with a random seed of 42 and keeping 1/3rd of the data for testing\n",
    "2. Fit a [decision tree regressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) to predict price using all the data (your dummy variables plus bedrooms and bathrooms).\n",
    "5. Report the root MSE on the test data.\n",
    "6. How does the tree decide on a splitting point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = data('Housing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>lotsize</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrms</th>\n",
       "      <th>stories</th>\n",
       "      <th>driveway</th>\n",
       "      <th>recroom</th>\n",
       "      <th>fullbase</th>\n",
       "      <th>gashw</th>\n",
       "      <th>airco</th>\n",
       "      <th>garagepl</th>\n",
       "      <th>prefarea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42000</td>\n",
       "      <td>5850</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38500</td>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49500</td>\n",
       "      <td>3060</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60500</td>\n",
       "      <td>6650</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>61000</td>\n",
       "      <td>6360</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  lotsize  bedrooms  bathrms  stories  driveway  recroom  fullbase  \\\n",
       "1  42000     5850         3        1        2         1        0         1   \n",
       "2  38500     4000         2        1        1         1        0         0   \n",
       "3  49500     3060         3        1        1         1        0         0   \n",
       "4  60500     6650         3        1        2         1        1         0   \n",
       "5  61000     6360         2        1        1         1        0         0   \n",
       "\n",
       "   gashw  airco  garagepl  prefarea  \n",
       "1      0      0         1         0  \n",
       "2      0      0         0         0  \n",
       "3      0      0         0         0  \n",
       "4      0      0         0         0  \n",
       "5      0      0         0         0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def yes_no_to_dummy(b):\n",
    "    if type(b) is str: return 1 if b == 'yes' else 0\n",
    "    else: return b\n",
    "\n",
    "to_dummyify = ['driveway', 'recroom', 'fullbase', 'gashw', 'airco', 'prefarea']\n",
    "for feature in to_dummyify: df[feature] = df[feature].apply(yes_no_to_dummy)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['lotsize', 'bedrooms', 'bathrms', 'stories', 'driveway', 'recroom', 'fullbase', 'gashw', 'airco', 'garagepl', 'prefarea']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df[features], df['price'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regr = DecisionTreeRegressor()\n",
    "regr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root MSE on test data: 24661.155790872454\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "score = mean_squared_error(Y_test, regr.predict(X_test))\n",
    "print(\"Root MSE on test data: {}\".format(sqrt(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes split by choosing the best split that has the lowest MSE. It looks across all features and at all possible splits, where it calcuates the mean of the data on either side of the split and then calcuates the MSE based on that. The one with the lowest MSE will be the chosen split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 (2+5+3=10pts) - Support Vector Regression\n",
    "\n",
    "This question will continue to use the data from question 3 - including the training and test split data.\n",
    "\n",
    "1. Fit a support vector regression using the standard options on [sklearn's SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). Report the root MSE.\n",
    "2. Repeat question 1 except use grid search to select the best value of C within this set: [0.001, 0.01, 0.1, 1,5,10,100] and try both a radial and polynomial kernel (thus trying 14 combinations). Choose the combination that minimizes MSE. Use 5 folds. Report the best model and the test root MSE.\n",
    "4. Discuss your results from parts 1 and 2 and how they differ from Question 3 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root MSE on test data: 29015.40778214684\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "Y_pred = SVR().fit(X_train, Y_train).predict(X_test)\n",
    "score = mean_squared_error(Y_test, Y_pred)\n",
    "print(\"Root MSE on test data: {}\".format(sqrt(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "search_params = [{'C':[0.001, 0.01, 0.1, 1, 5, 10, 100], 'kernel':['rbf','poly']}]\n",
    "classifier = GridSearchCV(SVR(), search_params, scoring=mse_scorer, cv=5, n_jobs=-1)\n",
    "classifier = classifier.fit(X_train, Y_train)\n",
    "\n",
    "Y_predict = classifier.predict(X_test)\n",
    "\n",
    "score = mean_squared_error(Y_test, regr.predict(X_test))\n",
    "print(\"MSE on test data: {}\".format(sqrt(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (3+2+2+2+2+2+2=15pts) - Random Forest\n",
    "\n",
    "This question will also continue to use the data from Question 1.\n",
    "1. Fit a random forest model grid searching over the following values: {'n_estimators':[10, 100, 1000], 'max_features': ['auto', 'sqrt', 'log2']}. Choose the combination that maximizes the average per-class accuracy. Use 5 folds. Report the best model, the confusion matrix, the accuracy for the no affairs class, the affairs class, and the average per-class accuracy.\n",
    "2. What do the n_estimators and max_features parameters do?\n",
    "3. Report the features in order of importance based on the model used in part 1\n",
    "4. Repeat question 1 using an AdaBoostClassifier and grid search over the following values: {'n_estimators':[50, 500, 5000], 'learning_rate': [.001, .01, .1]}\n",
    "5. What does the learning_rate parameter do?\n",
    "6. Report the features in order of importance based on the model used in part 4\n",
    "7. Compare the results in part 1 and 4 and questions 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
